---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# fl

<!-- badges: start -->
<!-- badges: end -->

The goal of fl is to ...

## Installation

You can install the development version of fl like so:

``` r
# FILL THIS IN! HOW CAN PEOPLE INSTALL YOUR DEV PACKAGE?
```

## Example

The idea is that we obtain data sources for gdal that don't require any logic in R functions. 

In this case, the OISST files are augmented as VRT. We need

- declare the driver and the subdataset (we get NETCDF:<filename>:sst in this case)
- fill in the missing crs metadata (we  need a full VRT expansion)

In GDAL 3.7.0 we don't need vrt expansion, we would use

```
vrt://NETCDF:<filename>:sst?a_srs=OGC:CRS83
```

but for now, need some tricks here to keep the VRT generation fast (by templating) and small-ish (by removing metadata). 

Then we have data source strings ready for the warper!  Here we get a source for every month, and write to a GeoTIFF for each, using average resampling (the data are aggregated to 2.5 cells from 0.25). We can cross the dateline or the prime meridian, it doens't matter because the warper sorts that out. 



```{r example, eval=TRUE}
library(fl)
files <- oisst_daily_sst_files(seq(as.Date("1992-03-21"), by = "1 month", length.out = 12L * 580))
#files <- oisst_daily_sst_files()


## we can get a mean
ext <- c(-60, 50, -70, -30)
dm <- c(60, 30)
#mn <- vapour::gdal_raster_data(files$datasource, target_dim = dm, target_ext = ext)
## or vectorize over sources
#bands <- lapply(files$datasource, vapour::gdal_raster_data, target_dim = dm, target_ext = ext)

library(furrr)
options(parallelly.fork.enable = TRUE, future.rng.onMisuse = "ignore")
plan(multicore)
func <- function(.x) vapour::gdal_raster_dsn(.x, target_dim = dm, target_ext = ext, resample = "average")[[1]]

## or write to file
system.time({
flist <- future_map_chr(files$datasource, func) 
})
plan(sequential)

str(basename(flist))
# chr [1:374] "file90aa0142157a4.tif" "file90aa0453395b2.tif" "file90aa04bd28a87.tif" "file90aa05fb4b65d.tif" ...
```



On 32 cores that takes 27 seconds, 374 input files. 


For the entire series, 15216 files it takes  162 seconds. 





Just to see what we get. 

```{r terra}
library(terra)
r <- rast(flist[seq(1, length(flist), length.out = 374)])
mn <- mean(r)
plot(mn - rast(sample(flist, 1)))

```

The data natively are in 0,360 - we can target any grid we want from this improved source collection. 


How long does it take to process to southern hemisphere 10000m. 


```{r laea, eval=TRUE}
library(fl)
files <- oisst_daily_sst_files(seq(as.Date("1992-03-21"), by = "1 month", length.out = 12L * 580))
#files <- oisst_daily_sst_files()


## we can get a mean
ext <- c(-1, 1, -1, 1) * 6378137 * pi/1.2
res <- rep(25000, 2L)
#mn <- vapour::gdal_raster_data(files$datasource, target_dim = dm, target_ext = ext)
## or vectorize over sources
#bands <- lapply(files$datasource, vapour::gdal_raster_data, target_dim = dm, target_ext = ext)


library(furrr)
options(parallelly.fork.enable = TRUE, future.rng.onMisuse = "ignore")
plan(multicore)
op <- options(warn = 1)
func <- function(.x) vapour::gdal_raster_dsn(.x, target_res = res, target_ext = ext, target_crs = "EPSG:3031", resample = "average")[[1]]

## or write to file
system.time({
flist <- future_map_chr(files$datasource, func) 
})
plan(sequential)

str(basename(flist))
# chr [1:374] "file90aa0142157a4.tif" "file90aa0453395b2.tif" "file90aa04bd28a87.tif" "file90aa05fb4b65d.tif" ...

terra::rast(flist[1])
terra::plot(terra::rast(flist[1]))
options(op)
```

